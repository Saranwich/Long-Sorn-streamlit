import streamlit as st
import os
import json
import requests
from google.cloud import speech
import google.generativeai as genai
import time
import pandas as pd
import subprocess
import tempfile
from collections import Counter
from google.oauth2 import service_account

# --- Page Configuration & ENV Loading ---
st.set_page_config(page_title="LongSorn AI Demo", page_icon="üñäÔ∏è", layout="wide")

# --- Backend Functions (AI Calls) ---
def get_audio_duration(file_path):
    """‡πÉ‡∏ä‡πâ ffprobe ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á/‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠"""
    command = ["ffprobe", "-v", "error", "-show_entries", "format=duration", "-of", "default=noprint_wrappers=1:nokey=1", file_path]
    try:
        result = subprocess.run(command, check=True, capture_output=True, text=True)
        return float(result.stdout.strip())
    except Exception as e:
        st.warning(f"Could not get audio duration: {e}")
        return 0

def convert_audio_with_ffmpeg(input_bytes, suffix, trim_duration=None):
    """‡πÉ‡∏ä‡πâ FFmpeg ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö WAV ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp_in:
            temp_in.write(input_bytes)
            input_filename = temp_in.name
        
        output_filename = input_filename + ".wav"
        command = ["ffmpeg", "-i", input_filename, "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", "-y"]
        if trim_duration:
            command.extend(["-t", str(trim_duration)])
        command.append(output_filename)

        subprocess.run(command, check=True, capture_output=True, text=True)
        
        with open(output_filename, "rb") as f:
            output_bytes = f.read()
            
        os.remove(input_filename)
        os.remove(output_filename)
        return output_bytes, None
    except Exception as e:
        st.error(f"FFmpeg Error: {e}")
        return None, str(e)

@st.cache_data
def run_stt_transcription(audio_file_content, language_code="th-TH"):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Google STT API ‡∏à‡∏£‡∏¥‡∏á (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏±‡πâ‡∏ô < 1 ‡∏ô‡∏≤‡∏ó‡∏µ)"""
    try:
        gcp_credentials_json = st.secrets["GCP_CREDENTIALS"]
        credentials_dict = json.loads(gcp_credentials_json)
        credentials = service_account.Credentials.from_service_account_info(credentials_dict)

        client = speech.SpeechClient(credentials=credentials)
        audio = speech.RecognitionAudio(content=audio_file_content)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code=language_code,
            enable_automatic_punctuation=True,
            enable_word_time_offsets=True,
        )
        response = client.recognize(config=config, audio=audio)
        return response, None
    except Exception as e:
        st.error(f"Google STT API Error: {e}")
        return None, str(e)

def find_timestamp_for_phrase(phrase, word_timestamps):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏ß‡∏•‡∏µ‡∏à‡∏≤‡∏Å word_timestamps"""
    clean_phrase = phrase.replace("...", "").strip()
    words_in_phrase = clean_phrase.lower().split()
    if not words_in_phrase: return "N/A"
    for i in range(len(word_timestamps) - len(words_in_phrase) + 1):
        match = True
        for j in range(len(words_in_phrase)):
            if word_timestamps[i+j]['Word'].lower() != words_in_phrase[j]:
                match = False; break
        if match:
            start_seconds = float(word_timestamps[i]['Start (s)'])
            minutes = int(start_seconds // 60); seconds = int(start_seconds % 60)
            return f"{minutes:01d}:{seconds:02d}"
    return "N/A"

def run_real_nlp_analysis(transcript: str, word_timestamps: list, description: str, lang_code_for_stt: str):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Transcript
    (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô 4 - Upgraded with Statistical Summary & Pause Analysis)
    """
    context_prompt = f"User's context for this presentation: {description}\n\n" if description else ""
    
    # --- Pre-calculation of all statistics ---
    word_count = len(word_timestamps)
    duration_seconds = float(word_timestamps[-1]['End (s)']) if word_timestamps and float(word_timestamps[-1]['End (s)']) > 0 else 1.0
    wpm = (word_count / duration_seconds) * 60 if duration_seconds > 0 else 0
    detected_language = lang_code_for_stt.split('-')[0].lower()

    # Filler word calculation
    if "th" in detected_language:
        fillers_list = ['‡πÄ‡∏≠‡πà‡∏≠', '‡∏≠‡πà‡∏≤', '‡∏Ñ‡∏∑‡∏≠', '‡πÅ‡∏ö‡∏ö‡∏ß‡πà‡∏≤', '‡πÅ‡∏ö‡∏ö', '‡∏Å‡πá‡∏Ñ‡∏∑‡∏≠', '‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏ô‡∏∞‡∏Ñ‡∏∞', '‡∏≠‡∏∞', '‡πÄ‡∏≠‡∏¥‡πà‡∏°', '‡∏≠‡∏∑‡∏°']
    else:
        fillers_list = ['um', 'uh', 'er', 'ah', 'like', 'actually', 'basically', 'so', 'you know', 'i mean', 'right']
    filler_word_count = sum(1 for word in transcript.lower().split() if word in fillers_list)

    # NEW: Pause Analysis
    long_pauses = 0
    pause_threshold = 2.0  # ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
    for i in range(1, len(word_timestamps)):
        pause_duration = word_timestamps[i]['Start (s)'] - word_timestamps[i-1]['End (s)']
        if pause_duration >= pause_threshold:
            long_pauses += 1
            
    # --- Construct the Data Summary for the AI ---
    data_summary = f"""
    Here is a statistical summary of the speech delivery:
    - Speaking Pace: {wpm:.0f} words per minute.
    - Filler Words Count: {filler_word_count} times.
    - Long Pauses (>{pause_threshold}s): {long_pauses} times.
    """

    # --- Create the new "Smarter" Prompt ---
    gemini_feedback = "Not available"
    try:
        genai.configure(api_key=st.secrets["GOOGLE_GEMINI_API_KEY"]) #
        model = genai.GenerativeModel('gemini-1.5-flash')

        # NEW: The prompt now instructs the AI to use the data summary
        smarter_prompt = f"""
        {context_prompt}
        You are an expert speech coach reviewing a presentation. Your analysis MUST be based on BOTH the provided statistical summary and the full transcript.

        {data_summary}

        Based on ALL the information above (statistics and transcript), perform the following analysis:

        1. Overall Clarity Score (1-10): Provide a score and a justification that REFERENCES the statistical data. For example, if the pace is too fast or there are many pauses, mention it as a reason for a lower score.
        - A clear, competent speaker should score 7-8. Reserve scores below 5 for speakers who are genuinely hard to follow.
        Use this exact format:
        Clarity: [Your Score] | Justification: [Your brief reason for the score, referencing the statistics]

        2. Key Improvement Suggestions: Identify up to 5 specific phrases or moments from the transcript that could be improved.
        Use this exact format, with each entry on a new line:
        ORIGINAL: [original phrase] | REASON: [reason for improvement] | SUGGESTION: [suggested alternative]

        3. Main Keywords: Extract up to 5 main keywords or topics.
        Use this exact format:
        KEYWORDS: [keyword1, keyword2, keyword3]
        """
        
        full_prompt = f"{smarter_prompt}\n\nFull Transcript:\n\"\"\"\n{transcript}\n\"\"\""
        
        response = model.generate_content(full_prompt)
        gemini_feedback = response.text
    except Exception as e:
        st.warning(f"Could not connect to Gemini API: {e}")

    # --- Parsing the results (mostly the same) ---
    clarity = 0.0
    clarity_justification = "N/A"
    keywords = []
    timeline_feedback = []
    ai_recommendations = []
    
    if wpm < 60: pace = "Very Slow"
    elif wpm < 100: pace = "Relaxed Pace"
    elif wpm <= 140: pace = "Good Conversational Pace"
    elif wpm <= 180: pace = "Energetic Pace"
    else: pace = "Very Fast"

    for line in gemini_feedback.splitlines():
        if "Clarity:" in line and "Justification:" in line:
            try:
                parts = line.split('|'); clarity_str = parts[0].replace("Clarity:", "").strip(); clarity = float(clarity_str)
                clarity_justification = parts[1].replace("Justification:", "").strip()
            except:
                clarity = 0.0; clarity_justification = "Could not parse score."
        elif "KEYWORDS:" in line:
            keywords_str = line.split("KEYWORDS:")[1].strip().replace('[', '').replace(']', ''); keywords = [k.strip() for k in keywords_str.split(',') if k.strip()]
        elif "ORIGINAL:" in line:
            parts = [p.strip() for p in line.split('|')]
            if len(parts) == 3:
                original = parts[0].replace("ORIGINAL:", "").strip(); reason = parts[1].replace("REASON:", "").strip(); suggestion = parts[2].replace("SUGGESTION:", "").strip()
                ai_recommendations.append({"original": original, "suggestion": suggestion})
                timestamp = find_timestamp_for_phrase(original, word_timestamps)
                timeline_feedback.append({"timestamp": timestamp, "type": reason, "suggestion": suggestion})

    return {
        "speech_analysis": {
            "Filler Words Detected": filler_word_count, "Speaking Pace": pace, "Clarity Score": clarity,
            "Clarity Justification": clarity_justification, "Long Pauses": long_pauses
        },
        "keywords": keywords, "timeline_feedback": timeline_feedback, "ai_recommendations": ai_recommendations
    }

# --- Main UI and Processing Logic ---
st.title("üñäÔ∏è LongSorn AI Demo")
st.caption("‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á AI ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ UI ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏à‡∏£‡∏¥‡∏á")
st.divider()

if 'results_ready' in st.session_state and st.session_state.results_ready:
    # --- UI: ‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ---
    st.header("AI Analysis Results")
    if st.session_state.get("is_trimmed", False):
        st.warning("‚ö†Ô∏è ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 60 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡πÅ‡∏£‡∏Å‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏ï‡πá‡∏° ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏û‡πá‡∏Å‡πÄ‡∏Å‡∏à")

    nlp_res = st.session_state.nlp_results
    
    left_col, right_col = st.columns(2, gap="large")

    with left_col:
        st.subheader("Video Playback")
        st.video(st.session_state.uploaded_file_content)
        
        st.subheader("Timeline Feedback")
        if nlp_res["timeline_feedback"]:
            for feedback in nlp_res["timeline_feedback"]:
                with st.container(border=True):
                    r1_col1, r1_col2 = st.columns([1, 4])
                    with r1_col1: st.write(f"**{feedback['timestamp']}**")
                    with r1_col2: st.write(f"**{feedback['type']}**")
                    st.info(f"**Suggestion:** {feedback['suggestion']}")

    with right_col:
        st.subheader("Speech Analysis")
        with st.container(border=True):
            analysis = nlp_res["speech_analysis"]
            col1, col2, col3 = st.columns(3)
            col1.metric("Speaking Pace", analysis.get('Speaking Pace', 'N/A'))
            col2.metric("Filler Words", f"{analysis.get('Filler Words Detected', 0)} times")
            col3.metric("Long Pauses (>2s)", f"{analysis.get('Long Pauses', 0)} times")
            
            st.metric("Clarity Score", f"{analysis.get('Clarity Score', 0.0):.1f} / 10", help=analysis.get('Clarity Justification', 'No justification provided.'))
        
        st.subheader("Content Analysis")
        with st.container(border=True):
            st.write("**Main Keywords:**")
            if nlp_res["keywords"]:
                st.text(", ".join(nlp_res["keywords"]))
    
        st.subheader("AI Recommendations")
        if nlp_res["ai_recommendations"]:
            with st.expander("‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", expanded=True): # ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô expanded=False ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏ñ‡∏π‡∏Å‡∏ã‡πà‡∏≠‡∏ô‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô
                for rec in nlp_res["ai_recommendations"]:
                    st.error(f"**Original:** \"_{rec['original']}_\"")
                    st.success(f"**Suggestion:** \"_{rec['suggestion']}_\"")
                    st.divider()

    if st.button("Analyze Another"): st.session_state.clear(); st.rerun()

elif 'analysis_triggered' in st.session_state and st.session_state.analysis_triggered:
    # --- UI: ‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ---
    with st.container(border=True):
        st.subheader("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...")
        progress_bar = st.progress(0, text="Starting...")
        
        progress_bar.progress(10, text="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á...")
        file_suffix = os.path.splitext(st.session_state.file_name)[1]
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_suffix) as temp_in:
            temp_in.write(st.session_state.uploaded_file_content); input_filename = temp_in.name
        
        duration = get_audio_duration(input_filename)
        is_trimmed = duration > 60
        st.session_state.is_trimmed = is_trimmed
        os.remove(input_filename)

        trim_duration = 60 if is_trimmed else None
        if is_trimmed: progress_bar.progress(20, text="‡πÑ‡∏ü‡∏•‡πå‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡∏ô‡∏≤‡∏ó‡∏µ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 60 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ...")
        
        converted_audio, ffmpeg_error = convert_audio_with_ffmpeg(st.session_state.uploaded_file_content, file_suffix, trim_duration)
        if ffmpeg_error: st.error(f"FFmpeg Error: {ffmpeg_error}"); st.stop()
        
        # --- Language Detection Step ---
        progress_bar.progress(30, text="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤...")
        # A simple heuristic for language detection
        lang_code_for_stt = "th-TH" # Default to Thai
        if st.session_state.get("user_description", "").lower().strip() == "english":
            lang_code_for_stt = "en-US"

        progress_bar.progress(40, text=f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°... ({lang_code_for_stt})...")
        stt_response, stt_error = run_stt_transcription(converted_audio, lang_code_for_stt)
        if stt_error: st.error(f"STT Error: {stt_error}"); st.stop()
        
        full_transcript = " ".join([res.alternatives[0].transcript for res in stt_response.results if res.alternatives])
        # --- NEW: Add this check for empty transcript ---
        if not full_transcript.strip():
            st.error("Error: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Ñ‡∏≥‡∏û‡∏π‡∏î‡πÉ‡∏î‡πÜ ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
            st.stop() # Stop the execution
        word_timestamps = []
        for result in stt_response.results:
            for word_info in result.alternatives[0].words:
                word_timestamps.append({
                    "Word": word_info.word,
                    "Start (s)": word_info.start_time.total_seconds(),
                    "End (s)": word_info.end_time.total_seconds()
                })
        st.session_state.word_timestamps_df = pd.DataFrame(word_timestamps)

        progress_bar.progress(70, text="‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤...")
        nlp_results = run_real_nlp_analysis(full_transcript, word_timestamps, st.session_state.get("user_description", ""), lang_code_for_stt)
        st.session_state.nlp_results = nlp_results
        
        progress_bar.progress(100, text="‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
        time.sleep(1)
        
        st.session_state.analysis_triggered = False
        st.session_state.results_ready = True
        st.rerun()

else:
    # --- UI: ‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ---
    with st.container(border=True):
        st.header("Upload Your Content")
        st.subheader("Provide context for AI")
        st.text_area("‡∏ö‡∏≠‡∏Å AI ‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏∞‡πÑ‡∏£ ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏© (‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÉ‡∏´‡πâ‡∏û‡∏¥‡∏°‡∏û‡πå 'english' ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà)", key="user_description", placeholder="e.g. ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô, ‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ")
        
        st.subheader("Upload your file")
        uploaded_file = st.file_uploader("Click to upload or drag and drop", type=["mp4", "mov", "mp3", "wav", "m4a"], label_visibility="collapsed")

        if uploaded_file:
            st.info(f"Selected File: **{uploaded_file.name}**")
            if st.button("Upload & Analyze", type="primary", use_container_width=True):
                st.session_state.analysis_triggered = True
                st.session_state.uploaded_file_content = uploaded_file.getvalue()
                st.session_state.file_name = uploaded_file.name
                st.rerun()
